<?xml version="1.0" encoding="UTF-8"?><process version="10.5.000">
  <context>
    <input>
      <location>../data/testExampleSetQA</location>
    </input>
    <output/>
    <macros>
      <macro>
        <key>model</key>
        <value>Llama-3.2-3B-Instruct-UD-Q5_K_XL.gguf</value>
      </macro>
      <macro>
        <key>temperature</key>
        <value>0.7</value>
      </macro>
      <macro>
        <key>top_k</key>
        <value>10</value>
      </macro>
      <macro>
        <key>top_p</key>
        <value>0.95</value>
      </macro>
      <macro>
        <key>min_p</key>
        <value>0.05</value>
      </macro>
      <macro>
        <key>max_tokens</key>
        <value>4096</value>
      </macro>
      <macro>
        <key>repeat_penalty</key>
        <value>1.1</value>
      </macro>
    </macros>
  </context>
  <operator activated="true" class="process" compatibility="9.4.000" expanded="true" name="Process" origin="GENERATED_TUTORIAL">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="82" name="QA Operator (llama.cpp GGUF)" width="90" x="112" y="85">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;QA Operator (llama\.cpp GGUF)&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;temperature&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Value should be greater zero\.&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 0\.7&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;top_k&quot;,&#10;      &quot;type&quot;: &quot;integer&quot;,&#10;      &quot;description&quot;: &quot;Value should be greater zero&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 40&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;top_p&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;An example of a double parameter\.&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 0\.95&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;min_p&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;An example of a double parameter\.&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 0\.0&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;max_tokens&quot;,&#10;      &quot;type&quot;: &quot;integer&quot;,&#10;      &quot;description&quot;: &quot;Multiples of 2, larger value will allow QA to output more tokens\.&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 2048&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;repeat_penalty&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;An example of a double parameter\.&quot;,&#10;      &quot;optional&quot;: true,&#10;      &quot;value&quot;: 1\.1&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;data&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;model&quot;,&#10;      &quot;type&quot;: &quot;file&quot;&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;out&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;through&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.from pandas import DataFrame&#10;from llama_cpp import Llama&#10;&#10;&#10;# Mandatory main function\. This example expects a single input followed by the&#10;# parameter dictionary\.&#10;def rm_main(data, mod, parameters):&#10;&#9;responses = []&#10;&#9;model = Llama(model_path=mod\.name,n_ctx=8192,use_mmap=False) &#10;&#9;for prompt in data['prompt']\.to_list():&#10;&#9;&#9;conversation = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]&#10;&#9;&#9;model\.reset()&#10;&#9;&#9;res = model\.create_chat_completion(conversation, top_k=int(parameters['top_k']), top_p=float(parameters['top_p']), min_p=float(parameters['min_p']), temperature=float(parameters['temperature']), max_tokens=int(parameters['max_tokens']), repeat_penalty=float(parameters['repeat_penalty']))&#10;&#9;&#9;qa_response = res[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]&#10;&#9;&#9;responses\.append(qa_response)&#10;&#9;response = data\.copy()&#10;&#9;response['qa_response'] = responses&#10;&#9;&#10;&#9;return response, data&#10;&#10;&#10;"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="temperature" value="0.7"/>
        <parameter key="top_k" value="40"/>
        <parameter key="top_p" value="0.95"/>
        <parameter key="min_p" value="0.0"/>
        <parameter key="max_tokens" value="2048"/>
        <parameter key="repeat_penalty" value="1.1"/>
        <description align="center" color="transparent" colored="false" width="126">QA operator with editable python script, note: uses llama.cpp</description>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="68" name="Event Type Prediction (BERT)" width="90" x="313" y="85">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Event Type Prediction (BERT)&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;model_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;By default parameters are of type string\.&quot;,&#10;      &quot;optional&quot;: false&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;text_header_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;By default parameters are of type string\.&quot;,&#10;      &quot;optional&quot;: false&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;data&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ],&#10;  &quot;outputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;out&quot;,&#10;      &quot;type&quot;: &quot;table&quot;&#10;    }&#10;  ]&#10;}.import os&#10;from pandas import DataFrame&#10;from transformers import pipeline as tpipline&#10;from tqdm import tqdm&#10;&#10;&#10;def rm_main(data, parameters):&#10;&#9;predictions, scores = [], []&#10;&#9;model_path = os\.path\.join(os\.path\.dirname(os\.getcwd()), &quot;models&quot;, parameters[&quot;model_name&quot;]) &#10;&#9;model = tpipline(task=&quot;text-classification&quot;, model=model_path, batch_size=512)&#10;&#9;tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': 512}&#10;&#9;for text in data[parameters[&quot;text_header_name&quot;]]\.to_list():&#10;&#9;&#9;pred = model(text, **tokenizer_kwargs)[0]&#10;&#9;&#9;&#10;&#9;&#9;predictions\.append(pred[&quot;label&quot;])&#10;&#9;&#9;scores\.append(pred[&quot;score&quot;])&#10;&#10;&#9;data[&quot;event_class&quot;] = predictions&#10;&#9;data[&quot;event_score&quot;] = scores&#10;&#10;&#9;return data"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="model_name" value="relevance-model-distilled-31624953"/>
        <parameter key="text_header_name" value="tweet_text"/>
        <description align="center" color="transparent" colored="false" width="126">Event type prediction operator using transformer library</description>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="68" name="Start Flink Job (SSH)" width="90" x="514" y="85">
        <parameter key="editable" value="true"/>
        <parameter key="operator" value="{&#10;  &quot;name&quot;: &quot;Start Flink Job (SSH)&quot;,&#10;  &quot;dropSpecial&quot;: false,&#10;  &quot;parameters&quot;: [&#9;&#10;    {&#10;      &quot;name&quot;: &quot;docker_container_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Docker container name on remote&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;flink-2-flink-2-jm-1&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;ssh_host_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;SSH connection server name&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;server\.crexdata\.eu&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;ssh_user_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;SSH connection user name&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;ubuntu&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;flink_job_name&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Name of flink job&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;relevance-prediction-job-test-async&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;kafka_server&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Kafka boostrap server and port&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;server\.crexdata\.eu:9092&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;input_topic&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Input kafka topic with tweets&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;input&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;output_topic&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Output kafka topic after processing&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;output&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;filtered_topic&quot;,&#10;      &quot;type&quot;: &quot;string&quot;,&#10;      &quot;description&quot;: &quot;Output kafka topic to send filtered tweet with detected events&quot;,&#10;      &quot;optional&quot;: false,&#10;      &quot;value&quot;: &quot;mytopik3&quot;&#10;    },&#10;    {&#10;      &quot;name&quot;: &quot;offset_type&quot;,&#10;      &quot;type&quot;: &quot;category&quot;,&#10;      &quot;description&quot;: &quot;Use 'earliest' to process a whole topic from the beginning and 'latest' for new incoming messages\.&quot;,&#10;      &quot;categories&quot;: [&#10;        &quot;latest&quot;,&#10;        &quot;earliest&quot;&#10;      ],&#10;      &quot;value&quot;: &quot;latest&quot;&#10;    }&#10;  ],&#10;  &quot;inputs&quot;: [&#10;    {&#10;      &quot;name&quot;: &quot;sshkey&quot;,&#10;      &quot;type&quot;: &quot;file&quot;&#10;    }&#10;  ]&#10;}.import paramiko&#10;&#10;&#10;def rm_main(key, parameters):&#10;&#9;key_file_path = key\.name&#10;&#9;print(key_file_path)&#10;&#9;&#10;&#9;# The command you want to execute on the remote server&#10;&#9;remote_command = f&quot;docker exec {parameters['docker_container_name']} bash -c 'flink run -d -py /mnt/relevance_job_utils/relevance-job-testing-async\.py --jarfile /mnt/relevance_job_utils/flink-sql-connector-kafka-4\.0\.0-2\.0\.jar &quot; +\\&#10;&#9;f&quot;--kafka-server {parameters['kafka_server']} &quot; +\\&#10;&#9;f&quot;--input-topic {parameters['input_topic']} &quot; +\\&#10;&#9;f&quot;--output-topic {parameters['output_topic']} &quot; +\\&#10;&#9;f&quot;--output-filtered {parameters['filtered_topic']} &quot; +\\&#10;&#9;f&quot;--job-name {parameters['flink_job_name']} &quot; +\\&#10;&#9;f&quot;--offset-type {parameters['offset_type']}'&quot;&#10;&#10;&#9;private_key = paramiko\.RSAKey\.from_private_key_file(key_file_path)&#10;&#9;&#10;&#9;# Establish an SSH connection&#10;&#9;ssh = paramiko\.SSHClient()&#10;&#9;ssh\.set_missing_host_key_policy(paramiko\.AutoAddPolicy())&#10;&#9;ssh\.connect(parameters['ssh_host_name'], username=parameters['ssh_user_name'], pkey=private_key)&#10;&#9;&#10;&#9;# Execute the remote command&#10;&#9;_, stdout, stderr = ssh\.exec_command(remote_command)&#10;&#9;print(&quot;Output of the remote command:&quot;)&#10;&#10;&#9;print(f&quot;stdout:\\n {stdout\.read()\.decode()}&quot;)&#10;&#9;ssh\.close()&#10;"/>
        <parameter key="use_default_python" value="false"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="conda_environment" value="rm_genai"/>
        <parameter key="docker_container_name" value="flink-2-flink-2-jm-1"/>
        <parameter key="ssh_host_name" value="server.crexdata.eu"/>
        <parameter key="ssh_user_name" value="ubuntu"/>
        <parameter key="flink_job_name" value="relevance-prediction-job-emcase-trials-noauth"/>
        <parameter key="kafka_server" value="server.crexdata.eu:9192"/>
        <parameter key="input_topic" value="UPB-CREXDATA-SocialMedia-Stream"/>
        <parameter key="output_topic" value="text_mining_processed_tweets_stream"/>
        <parameter key="filtered_topic" value="text_mining_processed_relevant_tweets_stream"/>
        <parameter key="offset_type" value="latest"/>
        <description align="center" color="transparent" colored="false" width="126">Start flink job operator using paramiko library for ssh</description>
      </operator>
      <operator activated="true" class="python_scripting:python_transformer" compatibility="10.1.002" expanded="true" height="82" name="Generate QA prompt" width="90" x="715" y="85">
        <parameter key="editable" value="false"/>
        <parameter key="operator" value="{&#13;&#10;  &quot;name&quot; : &quot;Custom Python Transformer&quot;,&#13;&#10;  &quot;dropSpecial&quot; : true,&#13;&#10;  &quot;parameters&quot; : [ {&#13;&#10;    &quot;name&quot; : &quot;summarise&quot;,&#13;&#10;    &quot;type&quot; : &quot;boolean&quot;,&#13;&#10;    &quot;description&quot; : &quot;Summarise the incident\. If True, query parameter not required\.&quot;,&#13;&#10;    &quot;value&quot; : true&#13;&#10;  }, {&#13;&#10;    &quot;name&quot; : &quot;query&quot;,&#13;&#10;    &quot;type&quot; : &quot;string&quot;,&#13;&#10;    &quot;description&quot; : &quot;An example of a categorical parameter\.&quot;,&#13;&#10;    &quot;value&quot; : &quot;Tell me about the incident in the messages\.&quot;,&#13;&#10;    &quot;optional&quot; : true&#13;&#10;  }, {&#13;&#10;    &quot;name&quot; : &quot;qa_response_language&quot;,&#13;&#10;    &quot;type&quot; : &quot;category&quot;,&#13;&#10;    &quot;description&quot; : &quot;Choose a language to receive response in\. English, Spanish, Catalan, German&quot;,&#13;&#10;    &quot;categories&quot; : [ &quot;English&quot;, &quot;Spanish&quot;, &quot;Catalan&quot;, &quot;German&quot; ],&#13;&#10;    &quot;value&quot; : &quot;English&quot;&#13;&#10;  } ],&#13;&#10;  &quot;inputs&quot; : [ {&#13;&#10;    &quot;name&quot; : &quot;data&quot;,&#13;&#10;    &quot;type&quot; : &quot;table&quot;&#13;&#10;  } ],&#13;&#10;  &quot;outputs&quot; : [ {&#13;&#10;    &quot;name&quot; : &quot;out&quot;,&#13;&#10;    &quot;type&quot; : &quot;table&quot;&#13;&#10;  }, {&#13;&#10;    &quot;name&quot; : &quot;through&quot;,&#13;&#10;    &quot;type&quot; : &quot;table&quot;&#13;&#10;  } ]&#13;&#10;}.from pandas import DataFrame&#10;&#10;# Mandatory main function\. This example expects a single input followed by the&#10;# parameter dictionary\.&#10;def rm_main(data, parameters):&#10;&#9;template_texts = &quot;&quot;&#10;&#9;for i, text in enumerate(data['tweet_text']\.to_list()):&#10;&#9;&#9;template_texts += f'{i+1}\. {text} \\n'&#10;&#10;&#9;if parameters['summarise']:&#10;&#9;&#9;prompt = &quot;The documents below describe a developing disaster event\. &quot;+\\&#10;&#9;&#9;&#9;    &quot;Based on these documents, write a brief summary in the form of a paragraph, &quot;+\\&#10;&#9;&#9;&#9;    &quot;highlighting the most crucial information\. &quot;+\\&#10;&#9;              f&quot;Reply ONLY in {parameters['qa_response_language']} regardless of the language in the documents\.\\n&quot;+\\&#10;&#9;              f&quot;Documents: {template_texts\.strip()}&quot;&#10;&#9;else:&#10;&#9;&#9;prompt = &quot;For the following query and documents, &quot;+\\&#10;&#9;              &quot;try to answer the given query using only the documents\. &quot;+\\&#10;&#9;              &quot;If the answer is not in the documents, state that\. &quot;+\\&#10;&#9;              &quot;Refer to documents in your response using their numbers\. &quot;+\\&#10;&#9;              f&quot;Reply ONLY in {parameters['qa_response_language']} regardless of the language in the documents\. &quot;+\\&#10;&#9;              f&quot;\\nQuery:\\n{parameters['query']}\\nDocuments:\\n{template_texts\.strip()}&quot;&#10;&#9;&#9;&#10;&#9;table = DataFrame({&#10;&#9;&#9;'prompt': [prompt]&#10;&#9;})&#10;&#10;&#9;return table, data&#10;"/>
        <parameter key="use_default_python" value="false"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <parameter key="conda_environment" value="rm_genai"/>
        <parameter key="summarise" value="true"/>
        <parameter key="query" value="Tell me about the incident in the messages."/>
        <parameter key="qa_response_language" value="English"/>
        <description align="center" color="transparent" colored="false" width="126">Creates QA prompt</description>
      </operator>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
    </process>
  </operator>
</process>
